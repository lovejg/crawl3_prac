import time
import datetime
import mysql.connector
from mysql.connector import Error
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from fake_useragent import UserAgent

# DB ì„¤ì •
db_config = {
    'host': 'localhost',
    'user': 'root',
    'password': '1234',
    'database': 'recruit',
    'port': 3307
}

def create_connection(config):
    try:
        conn = mysql.connector.connect(**config)
        print("âœ… DB ì—°ê²° ì„±ê³µ")
        return conn
    except Error as e:
        print(f"DB ì—°ê²° ì‹¤íŒ¨: {e}")
        return None

def insert_job(cursor, data):
    query = """
        INSERT INTO recruitment 
        (title, company_name, company_location, require_career, require_skill, 
         detail_url, detail, expire_date)
        VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
    """
    try:
        cursor.execute(query, data)
        print(f"ğŸ“¥ ì €ì¥ ì„±ê³µ: {data[0]}")
    except Error as e:
        print(f"âŒ ì €ì¥ ì‹¤íŒ¨: {e}")

# ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§
def crawl_job_detail(detail_url):
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument(f"user-agent={UserAgent().chrome}")
    driver = None

    try:
        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
        driver.get(detail_url)
        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, "section.JobContent_JobContent__Qb6DR")))
        time.sleep(2)

        html_source = driver.page_source
        soup = BeautifulSoup(html_source, 'html.parser')
        html = soup.select_one("section.JobContent_JobContent__Qb6DR")
        main = html.select_one("section.JobContent_descriptionWrapper__RMlfm")

        # ê¸°ìˆ  ìŠ¤íƒ
        tags_article = soup.select_one("article.CompanyTags_CompanyTags__OpNto")
        tag_buttons = tags_article.select("button[data-attribute-id='company__tag__click']") if tags_article else []
        skills = ', '.join([btn.get('data-tag-name') for btn in tag_buttons if btn.get('data-tag-name')])

        # ìƒì„¸ ì •ë³´
        details = main.select("span.wds-h4ga6o")
        full_detail = "\n".join([d.text.strip() for d in details])

        # ë§ˆê°ì¼
        deadline = main.select_one("span.wds-1u1yyy")
        expire = deadline.text.strip() if deadline else 'ì •ë³´ ì—†ìŒ'

        return skills, full_detail, expire

    except Exception as e:
        print(f"âŒ ìƒì„¸ í˜ì´ì§€ ì˜¤ë¥˜: {e}")
        return '', '', ''
    finally:
        if driver:
            driver.quit()

# ë©”ì¸ í¬ë¡¤ë§
def crawl_wanted(keyword):
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument(f"user-agent={UserAgent().chrome}")

    driver = None
    seen_links = set()
    results = []

    try:
        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
        driver.get(f"https://www.wanted.co.kr/search?query={keyword}&tab=position")
        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, 'div.JobCard_container__zQcZs')))
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")

        items = driver.find_elements(By.CSS_SELECTOR, 'div.JobCard_container__zQcZs')

        for item in items:
            try:
                title = item.find_element(By.CSS_SELECTOR, 'strong.JobCard_title___kfvj').text.strip()
                info = item.find_elements(By.CSS_SELECTOR, 'span.wds-nkj4w6')
                if len(info) < 2:
                    continue

                company = info[0].text.strip()
                career = info[1].text.strip()
                link = item.find_element(By.TAG_NAME, 'a').get_attribute('href')

                if link in seen_links:
                    continue
                seen_links.add(link)

                skills, detail, expire = crawl_job_detail(link)

                # íšŒì‚¬ ìœ„ì¹˜ëŠ” ìƒì„¸ í˜ì´ì§€ì—ì„œ ê°™ì´ ê°€ì ¸ì˜¬ ìˆ˜ë„ ìˆìŒ â†’ ì¼ë‹¨ì€ "ì •ë³´ ì—†ìŒ"
                results.append((title, company, 'ì •ë³´ ì—†ìŒ', career, skills, link, detail, expire))

            except Exception as e:
                print(f"âš ï¸ ê°œë³„ í•­ëª© ì˜¤ë¥˜: {e}")
                continue

    except Exception as e:
        print(f"âŒ ë©”ì¸ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
    finally:
        if driver:
            driver.quit()
    return results

# ë©”ì¸ ì‹¤í–‰
def main():
    keyword = input("í‚¤ì›Œë“œ ì…ë ¥: ").strip()
    print(f"\nğŸ” '{keyword}' í‚¤ì›Œë“œë¡œ ì›í‹°ë“œ í¬ë¡¤ë§ ì‹œì‘")
    conn = create_connection(db_config)
    if not conn:
        return
    cursor = conn.cursor()

    jobs = crawl_wanted(keyword)
    print(f"\nâœ… ì´ {len(jobs)}ê°œì˜ ê³µê³  ìˆ˜ì§‘ë¨")

    for job in jobs:
        insert_job(cursor, job)

    conn.commit()
    cursor.close()
    conn.close()
    print("\nğŸ‰ ëª¨ë“  ë°ì´í„° ì €ì¥ ì™„ë£Œ. DB ì—°ê²° ì¢…ë£Œ.")

if __name__ == "__main__":
    main()
