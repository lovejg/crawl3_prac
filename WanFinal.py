import time
import datetime
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from fake_useragent import UserAgent
import mysql.connector
from mysql.connector import Error

# --- 1. DB ì„¤ì • ë° ì—°ê²° í•¨ìˆ˜ ---
db_config = {
    'host': 'localhost',
    'user': 'root',
    'password': '1234',
    'database': 'recruit',
    'port': 3307,
    'charset': 'utf8mb4'
}

def create_connection(config):
    """DB ì—°ê²° ìƒì„±"""
    connection = None
    try:
        connection = mysql.connector.connect(**config)
        print("ğŸ‰ MySQL DBì— ì„±ê³µì ìœ¼ë¡œ ì—°ê²°ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Error as e:
        print(f"DB ì—°ê²° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    return connection

def insert_job_data(cursor, data):
    """í¬ë¡¤ë§í•œ ë°ì´í„°ë¥¼ DBì— ì‚½ì…"""
    query = """
        INSERT INTO recruitment
        (title, company_name, company_location, require_career, detail_url, detail, expire_date)
        VALUES (%s, %s, %s, %s, %s, %s, %s)
        ON DUPLICATE KEY UPDATE
            title=VALUES(title), company_location=VALUES(company_location),
            detail=VALUES(detail), expire_date=VALUES(expire_date);
    """
    try:
        cursor.execute(query, data)
        print(f"  âœ… [DB ì €ì¥/ì—…ë°ì´íŠ¸ ì™„ë£Œ] {data[0]}")
    except Error as e:
        print(f"  âŒ [DB ì €ì¥ ì‹¤íŒ¨] {data[0]} - {e}")

def scrape_detail_page(driver, url):
    try:
        driver.get(url)
        WebDriverWait(driver, 15).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "section.JobContent_JobContent__Qb6DR"))
        )
        time.sleep(3)

        try:
            buttons = driver.execute_script("""
                return Array.from(document.querySelectorAll('button')).filter(
                    btn => btn.textContent.includes('ë”ë³´ê¸°') || btn.textContent.includes('ìƒì„¸ì •ë³´')
                );
            """)
            if buttons:
                driver.execute_script("arguments[0].click();", buttons[0])
                time.sleep(3)
        except Exception as e:
            print(f"    ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­ ì‹¤íŒ¨: {e}")

        soup = BeautifulSoup(driver.page_source, 'html.parser')
        main_content = soup.select_one("section.JobContent_descriptionWrapper__RMlfm")
        header_content = soup.select_one("header.JobHeader_JobHeader__TZkW3")
        if not main_content or not header_content:
            print("    ì£¼ìš” ì»¨í…ì¸  ì˜ì—­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None

        details = {}

        # ì œëª©
        title_elem = header_content.select_one("h1.wds-58fmok")
        details['title'] = title_elem.text.strip() if title_elem else ''

        # íšŒì‚¬ëª…
        company_elem = header_content.select_one("a.JobHeader_JobHeader__Tools__Company__Link__NoBQI")
        details['company'] = company_elem.text.strip() if company_elem else ''

        # ê²½ë ¥
        career_elem = header_content.select("span.JobHeader_JobHeader__Tools__Company__Info__b9P4Y")[1]
        details['career'] = career_elem.text.strip() if (career_elem) else ''

        # ë§ˆê°ì¼
        expire_elem = main_content.select_one("span.wds-1u1yyy")
        details['expire_date'] = expire_elem.text.strip() if expire_elem else ''
        
        # ìœ„ì¹˜
        location_elem = main_content.select_one("span.wds-1td1qmv")
        details['location'] = location_elem.text.strip() if location_elem else ''

         # âœ… ìƒì„¸ ì •ë³´ íŒŒíŠ¸ë³„ ì¶”ì¶œ
        part_sections = main_content.select("section[class*='JobContent'] span[class*='wds-']")
        section_labels = ['í¬ì§€ì…˜ ìƒì„¸', 'ì£¼ìš”ì—…ë¬´', 'ìê²©ìš”ê±´', 'ìš°ëŒ€ì‚¬í•­', 'í˜œíƒ ë° ë³µì§€', 'ì±„ìš© ì „í˜•']
        section_data = {}

        for i, label in enumerate(section_labels):
            section_data[label] = part_sections[i].text.strip() if i < len(part_sections) else 'ì •ë³´ ì—†ìŒ'

        # âœ… íšŒì‚¬ íƒœê·¸
        try:
            tags_article = soup.select_one("article.CompanyTags_CompanyTags__OpNto")
            if tags_article:
                tag_buttons = tags_article.select("button[data-attribute-id='company__tag__click']")
                tags_list = [btn.get('data-tag-name') for btn in tag_buttons if btn.get('data-tag-name')]
                section_data['íšŒì‚¬ íƒœê·¸'] = ', '.join(tags_list) if tags_list else 'ì •ë³´ ì—†ìŒ'
            else:
                section_data['íšŒì‚¬ íƒœê·¸'] = 'ì •ë³´ ì—†ìŒ'
        except:
            section_data['íšŒì‚¬ íƒœê·¸'] = 'ì •ë³´ ì—†ìŒ'
            
        # âœ… detail ë¬¸ìì—´ë¡œ ì¡°ë¦½
        detail_text = ""
        for label in section_labels + ['íšŒì‚¬ íƒœê·¸']:
            detail_text += f"## {label}\n{section_data[label]}\n\n"

        details['detail_text'] = detail_text.strip()

        return details

    except Exception as e:
        print(f"    ìƒì„¸ í˜ì´ì§€ íŒŒì‹± ì˜¤ë¥˜: {e}")
        return None

# --- 3. ë©”ì¸ ì‹¤í–‰ ë¡œì§ ---
def main():
    keyword = input("ê²€ìƒ‰í•  í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”: ").strip()

    conn = create_connection(db_config)
    if not conn:
        return
    cursor = conn.cursor()

    options = Options()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    options.add_experimental_option('useAutomationExtension', False)
    options.add_argument(f"user-agent={UserAgent().chrome}")

    driver = None
    try:
        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
        driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")

        url = f"https://www.wanted.co.kr/search?query={keyword}&tab=position"
        driver.get(url)

        print("ì›í‹°ë“œ ëª©ë¡ í˜ì´ì§€ ë¡œë”© ë° ìŠ¤í¬ë¡¤ ì¤‘...")

        try:
            WebDriverWait(driver, 20).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, 'div.JobCard_container__zQcZs'))
            )
        except Exception as e:
            print(f"í˜ì´ì§€ ë¡œë”© ì‹¤íŒ¨: {e}")
            try:
                WebDriverWait(driver, 10).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, '[data-cy="job-card"]'))
                )
                print("ëŒ€ì•ˆ ì„ íƒìë¡œ í˜ì´ì§€ ë¡œë”© ì„±ê³µ")
            except:
                print("í˜ì´ì§€ ë¡œë”©ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                return

        for i in range(5):
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)
            print(f"ìŠ¤í¬ë¡¤ {i+1}/5 ì™„ë£Œ")

        selectors = [
            'div.JobCard_container__zQcZs',
            '[data-cy="job-card"]',
            'div[class*="JobCard"]'
        ]

        items = []
        for selector in selectors:
            items = driver.find_elements(By.CSS_SELECTOR, selector)
            if items:
                print(f"ì„ íƒì '{selector}'ë¡œ {len(items)}ê°œ ìš”ì†Œ ë°œê²¬")
                break

        if not items:
            print("ì±„ìš© ê³µê³ ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return

        job_links = []
        seen_links = set()

        for item in items:
            try:
                link_elem = item.find_element(By.TAG_NAME, 'a')
                link = link_elem.get_attribute('href')

                if link and link not in seen_links:
                    seen_links.add(link)
                    job_links.append({'url': link})
            except Exception as e:
                print(f"í•­ëª© íŒŒì‹± ì˜¤ë¥˜: {e}")
                continue

        print(f"\nì´ {len(job_links)}ê°œì˜ ì±„ìš© ê³µê³ ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ìƒì„¸ ì •ë³´ í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤.")

        if not job_links:
            print("ìˆ˜ì§‘ëœ ì±„ìš© ê³µê³ ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        for i, job in enumerate(job_links):
            print(f"\n({i+1}/{len(job_links)}) ìƒì„¸ ì •ë³´ í¬ë¡¤ë§ ì¤‘...")

            detail_data = scrape_detail_page(driver, job['url'])

            if detail_data:
                print(f"    â–¶ ê³µê³  ì œëª©: {detail_data.get('title', '')}")
                final_data = (
                    detail_data.get('title', ''),
                    detail_data.get('company', ''),
                    detail_data.get('location', ''),
                    detail_data.get('career', ''),
                    job['url'],
                    detail_data.get('detail_text', ''),
                    detail_data.get('expire_date', '')
                )
                insert_job_data(cursor, final_data)
            else:
                print(f"  âš ï¸ ìƒì„¸ ì •ë³´ í¬ë¡¤ë§ ì‹¤íŒ¨, ê¸°ë³¸ ì •ë³´ë§Œ ì €ì¥")
                basic_data = (
                    '', '', '', '', job['url'], '', ''
                )
                insert_job_data(cursor, basic_data)

            time.sleep(1)

    except Exception as e:
        print(f"ì „ì²´ í¬ë¡¤ë§ ê³¼ì •ì—ì„œ ì˜¤ë¥˜ ë°œìƒ: {e}")
    finally:
        if driver:
            driver.quit()
        if conn and conn.is_connected():
            conn.commit()
            cursor.close()
            conn.close()
            print("\nëª¨ë“  ì‘ì—… ì™„ë£Œ. DB ì—°ê²°ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
