import time
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from fake_useragent import UserAgent
from selenium.common.exceptions import TimeoutException
import mysql.connector
from mysql.connector import Error

# --- 1. DB ì„¤ì • ë° ì—°ê²° í•¨ìˆ˜ ---
db_config = {
    'host': 'localhost',
    'user': 'root',
    'password': '1234',
    'database': 'recruit',
    'port': 3307
}

def create_connection(config):
    """DB ì—°ê²° ìƒì„±"""
    connection = None
    try:
        connection = mysql.connector.connect(**config)
        print("ğŸ‰ MySQL DBì— ì„±ê³µì ìœ¼ë¡œ ì—°ê²°ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Error as e:
        print(f"DB ì—°ê²° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    return connection

def insert_job_data(cursor, data):
    """í¬ë¡¤ë§í•œ ë°ì´í„°ë¥¼ DBì— ì‚½ì…"""
    query = """
        INSERT INTO recruitment
        (title, company_name, company_location, is_regular, require_career, detail_url, detail)
        VALUES (%s, %s, %s, %s, %s, %s, %s)
        ON DUPLICATE KEY UPDATE
            title=VALUES(title), is_regular=VALUES(is_regular),
            require_career=VALUES(require_career), detail=VALUES(detail);
    """
    try:
        cursor.execute(query, data)
        print(f"  âœ… [DB ì €ì¥/ì—…ë°ì´íŠ¸ ì™„ë£Œ] {data[0]}")
    except Error as e:
        print(f"  âŒ [DB ì €ì¥ ì‹¤íŒ¨] {data[0]} - {e}")

# --- 2. ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§ í•¨ìˆ˜ (ìµœì í™”) ---
def scrape_detail_page(driver):
    """
    í˜„ì¬ ë“œë¼ì´ë²„ê°€ ìœ„ì¹˜í•œ í˜ì´ì§€ì˜ ìƒì„¸ ì •ë³´ë¥¼ í¬ë¡¤ë§.
    ë¹ ë¥¸ ì²˜ë¦¬ë¥¼ ìœ„í•´ ë‹¨ì¼ ì‹œë„ë¡œ ìµœì í™”
    """
    try:
        # í•µì‹¬ ìš”ì†Œ ë¡œë”© ëŒ€ê¸° (ì‹œê°„ ë‹¨ì¶•)
        time.sleep(2)
        
        # ì œëª© ì¶”ì¶œ - ê°€ì¥ í™•ì‹¤í•œ ì…€ë ‰í„°ë¶€í„° ì‹œë„
        title = None
        try:
            title_element = WebDriverWait(driver, 8).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, "span.eLNvYc"))
            )
            title = title_element.text.strip()
        except:
            # ë°±ì—… ë°©ë²•
            try:
                title_element = driver.find_element(By.TAG_NAME, "h1")
                title = title_element.text.strip()
            except:
                pass
        
        if not title:
            return None, None
        
        print(f"    ì œëª©: {title}")
        
        # ìƒì„¸ ì •ë³´ íŒŒì‹± - íš¨ìœ¨ì ì¸ ë°©ë²• ìš°ì„  ì‹œë„
        detail_text = ""
        
        try:
            # ë©”ì¸ ì½˜í…ì¸  ì˜ì—­ ì°¾ê¸°
            editor_element = driver.find_element(By.CSS_SELECTOR, "div.ql-editor")
            all_children = editor_element.find_elements(By.XPATH, "./*")
            
            if all_children:
                print(f"    êµ¬ì¡°í™”ëœ ì½˜í…ì¸  íŒŒì‹± ì¤‘... (ìš”ì†Œ ìˆ˜: {len(all_children)})")
                
                job_details_dict = {}
                current_section_title = None
                current_section_content = []
                
                # ì¤‘ìš”í•œ ì„¹ì…˜ë§Œ í•„í„°ë§
                important_keywords = [
                    "ë‹´ë‹¹ì—…ë¬´", "ì—…ë¬´", "ì—­í• ", "ì±…ì„", "ì£¼ìš”ì—…ë¬´",
                    "ìê²©ìš”ê±´", "ìš°ëŒ€ì‚¬í•­", "í•„ìˆ˜", "ìš°ëŒ€", "ìš”êµ¬ì‚¬í•­",
                    "ê·¼ë¬´ì¡°ê±´", "ê·¼ë¬´í™˜ê²½", "í˜œíƒ", "ë³µë¦¬í›„ìƒ",
                    "ì±„ìš©ì ˆì°¨", "ì „í˜•ì ˆì°¨", "ì§€ì›"
                ]

                for element in all_children:
                    if element.tag_name in ['h1', 'h2', 'h3', 'h4']:
                        # ì´ì „ ì„¹ì…˜ ì €ì¥ (ì¤‘ìš”í•œ ì„¹ì…˜ë§Œ)
                        if current_section_title and current_section_content:
                            if any(keyword in current_section_title for keyword in important_keywords):
                                job_details_dict[current_section_title] = "\n".join(current_section_content).strip()
                        
                        # ìƒˆ ì„¹ì…˜ ì‹œì‘
                        current_section_title = element.text.strip().replace("â¥", "").strip()
                        current_section_content = []
                    else:
                        text = element.text.strip()
                        if text and len(text) > 3:  # ë„ˆë¬´ ì§§ì€ í…ìŠ¤íŠ¸ ì œì™¸
                            current_section_content.append(text)
                
                # ë§ˆì§€ë§‰ ì„¹ì…˜ ì €ì¥
                if current_section_title and current_section_content:
                    if any(keyword in current_section_title for keyword in important_keywords):
                        job_details_dict[current_section_title] = "\n".join(current_section_content).strip()

                # êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸ ì¡°í•©
                if job_details_dict:
                    for key, value in job_details_dict.items():
                        if key and value:
                            detail_text += f"## {key}\n{value}\n\n"
                    print(f"    êµ¬ì¡°í™”ëœ ì½˜í…ì¸  ì¶”ì¶œ ì™„ë£Œ (ì„¹ì…˜ ìˆ˜: {len(job_details_dict)})")
                else:
                    print("    ì¤‘ìš”í•œ ì„¹ì…˜ì„ ì°¾ì§€ ëª»í•¨")
        
        except Exception as e:
            print(f"    êµ¬ì¡°í™” íŒŒì‹± ì‹¤íŒ¨: {e}")
        
        # êµ¬ì¡°í™”ëœ íŒŒì‹±ì´ ì‹¤íŒ¨í–ˆê±°ë‚˜ ë‚´ìš©ì´ ë¶€ì¡±í•œ ê²½ìš°ì—ë§Œ ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        if not detail_text.strip():
            print("    ì „ì²´ í˜ì´ì§€ í…ìŠ¤íŠ¸ ì¶”ì¶œë¡œ ì „í™˜")
            try:
                # í•µì‹¬ ì½˜í…ì¸  ì˜ì—­ë§Œ ì¶”ì¶œ
                main_content = driver.find_element(By.CSS_SELECTOR, "div.ql-editor")
                soup = BeautifulSoup(main_content.get_attribute('innerHTML'), 'html.parser')
                
                # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
                for unwanted in soup.find_all(['script', 'style', 'nav', 'header', 'footer']):
                    unwanted.decompose()
                
                # í…ìŠ¤íŠ¸ ì •ë¦¬
                text = soup.get_text()
                lines = [line.strip() for line in text.splitlines() if line.strip()]
                detail_text = '\n'.join(lines)
                
                # ê¸¸ì´ ì œí•œ (í•„ìš”í•œ ë¶€ë¶„ë§Œ)
                if len(detail_text) > 3000:
                    detail_text = detail_text[:3000] + "\n...(ì´í•˜ ìƒëµ)"
                    
            except Exception as e:
                print(f"    ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œë„ ì‹¤íŒ¨: {e}")
                return None, None
        
        if title and detail_text.strip():
            print(f"    âœ… ì„±ê³µ - ìƒì„¸ë‚´ìš© ê¸¸ì´: {len(detail_text)}")
            return title, detail_text.strip()
        else:
            return None, None
            
    except Exception as e:
        print(f"    âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        return None, None

# --- 3. ë©”ì¸ ì‹¤í–‰ ë¡œì§ ---
def main():
    conn = create_connection(db_config)
    if not conn:
        return
    cursor = conn.cursor()

    options = Options()
    options.add_argument("--headless") 
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    options.add_experimental_option('useAutomationExtension', False)
    options.add_argument(f"user-agent={UserAgent().chrome}")
    
    driver = None
    try:
        print("ì¹´ì¹´ì˜¤í˜ì´ ì±„ìš© ì •ë³´ í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
        driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        
        url = 'https://kakaopay.career.greetinghr.com/ko/main?occupations=ê¸°ìˆ '
        driver.get(url)
        main_window = driver.current_window_handle
        
        print("ë©”ì¸ í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° ì¤‘...")
        time.sleep(3)  # ëŒ€ê¸°ì‹œê°„ ë‹¨ì¶•

        # ìŠ¤í¬ë¡¤ ë‹¤ìš´ (ìµœì í™”)
        print("í˜ì´ì§€ ëê¹Œì§€ ìŠ¤í¬ë¡¤ ì¤‘...")
        last_height = driver.execute_script("return document.body.scrollHeight")
        scroll_attempts = 0
        max_scroll_attempts = 5  # íšŸìˆ˜ ì¤„ì„
        
        while scroll_attempts < max_scroll_attempts:
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(1.5)  # ëŒ€ê¸°ì‹œê°„ ë‹¨ì¶•
            new_height = driver.execute_script("return document.body.scrollHeight")
            if new_height == last_height:
                break
            last_height = new_height
            scroll_attempts += 1

        soup = BeautifulSoup(driver.page_source, 'html.parser')
        items = soup.select("ul.ffGmZN > a")
        
        if not items:
            print("ì±„ìš© ê³µê³  ëª©ë¡ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. í˜ì´ì§€ êµ¬ì¡°ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
            return

        jobs_to_crawl = []
        for item in items:
            try:
                link = f"https://kakaopay.career.greetinghr.com{item['href']}"
                info_items = item.select('.gAEjfw span')
                career = "ì •ë³´ ì—†ìŒ"
                work_type = "ì •ë³´ ì—†ìŒ"
                for info in info_items:
                    text = info.get_text(strip=True)
                    if 'ë…„' in text or 'ì‹ ì…' in text or 'ë¬´ê´€' in text:
                        career = text
                    elif 'ì •ê·œ' in text or 'ê³„ì•½' in text:
                        work_type = text
                jobs_to_crawl.append({'url': link, 'career': career, 'work_type': work_type})
            except Exception as e:
                print(f"ê³µê³  ì •ë³´ íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        print(f"\nì´ {len(jobs_to_crawl)}ê°œì˜ ì±„ìš© ê³µê³ ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ìƒì„¸ ì •ë³´ í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
        
        success_count = 0
        for i, job in enumerate(jobs_to_crawl):
            print(f"\n({i+1}/{len(jobs_to_crawl)}) ê³µê³  ì²˜ë¦¬ ì¤‘...")
            print(f"URL: {job['url']}")
            
            try:
                # ìƒˆ íƒ­ì—ì„œ ìƒì„¸ í˜ì´ì§€ ì—´ê¸°
                driver.switch_to.new_window('tab')
                driver.get(job['url'])
                
                detail_title, detail_text = scrape_detail_page(driver)

                # ì‘ì—… í›„ ìƒˆ íƒ­ ë‹«ê³  ë©”ì¸ íƒ­ìœ¼ë¡œ ëŒì•„ì˜¤ê¸°
                driver.close()
                driver.switch_to.window(main_window)

                if detail_title and detail_text:
                    final_data = (
                        detail_title, 'ì¹´ì¹´ì˜¤í˜ì´', 'ê²½ê¸°ë„ ì„±ë‚¨ì‹œ ë¶„ë‹¹êµ¬ íŒêµì—­ë¡œ 152 (ë°±í˜„ë™) ì•ŒíŒŒë”íƒ€ì›Œ 12ì¸µ',
                        job['work_type'], job['career'], job['url'], detail_text
                    )
                    insert_job_data(cursor, final_data)
                    success_count += 1
                else:
                    print(f"  - ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨. ê±´ë„ˆëœë‹ˆë‹¤.")
                    
            except Exception as e:
                print(f"  - ê³µê³  ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                # ì—ëŸ¬ ë°œìƒ ì‹œì—ë„ ë©”ì¸ íƒ­ìœ¼ë¡œ ëŒì•„ê°€ê¸°
                try:
                    driver.switch_to.window(main_window)
                except:
                    pass
        
        print(f"\ní¬ë¡¤ë§ ì™„ë£Œ: {success_count}/{len(jobs_to_crawl)}ê°œ ì„±ê³µ")

    except Exception as e:
        print(f"ì „ì²´ í¬ë¡¤ë§ ê³¼ì •ì—ì„œ ì˜¤ë¥˜ ë°œìƒ: {e}")
    finally:
        if driver:
            driver.quit()
        if conn and conn.is_connected():
            conn.commit()
            cursor.close()
            conn.close()
            print("\nëª¨ë“  ì‘ì—… ì™„ë£Œ. DB ì—°ê²°ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")

if __name__ == "__main__":
    main()